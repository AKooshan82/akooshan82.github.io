<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>World Models Blog - Complete Guide</title>

    <meta name="author" content="Ayeen Poostforoushan, Kooshan Fattah">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    
    <!-- MathJax for LaTeX rendering -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>
    <script>
      MathJax = {
        tex: {
          inlineMath: [['\\(', '\\)']],
          displayMath: [['\\[', '\\]']]
        }
      };
    </script>
    
    <style>
      body {
        font-family: 'Inter', 'Segoe UI', 'Roboto', sans-serif;
        line-height: 1.8;
        margin: 0;
        padding: 20px;
        background-color: #f8f9fa;
        color: #333;
      }
      
      .container {
        max-width: 1000px;
        margin: 0 auto;
        background: white;
        padding: 40px;
        border-radius: 15px;
        box-shadow: 0 4px 20px rgba(0,0,0,0.1);
      }
      
      h1 {
        color: #2c3e50;
        text-align: center;
        border-bottom: 4px solid #3498db;
        padding-bottom: 15px;
        font-size: 2.5em;
        margin-bottom: 30px;
      }
      
      h2 {
        color: #2980b9;
        margin-top: 40px;
        border-left: 6px solid #3498db;
        padding-left: 20px;
        font-size: 1.8em;
        margin-bottom: 20px;
      }
      
      h3 {
        color: #34495e;
        margin-top: 30px;
        font-size: 1.4em;
        border-bottom: 2px solid #ecf0f1;
        padding-bottom: 8px;
      }
      
      .author-info {
        text-align: center;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 25px;
        border-radius: 10px;
        margin: 30px 0;
        box-shadow: 0 4px 15px rgba(0,0,0,0.2);
      }
      
      .math-block {
        background: #f8f9fa;
        padding: 20px;
        margin: 20px 0;
        border-radius: 8px;
        border-left: 5px solid #e74c3c;
        font-size: 1.1em;
      }
      
      .algorithm-box {
        background: #fff5f5;
        border: 2px solid #e53e3e;
        border-radius: 8px;
        padding: 20px;
        margin: 20px 0;
      }
      
      .algorithm-title {
        font-weight: bold;
        color: #e53e3e;
        margin-bottom: 15px;
        font-size: 1.2em;
      }
      
      .key-insight {
        background: #f0fff4;
        border-left: 5px solid #48bb78;
        padding: 15px;
        margin: 20px 0;
        border-radius: 0 8px 8px 0;
        font-style: italic;
      }
      
      table {
        width: 100%;
        border-collapse: collapse;
        margin: 25px 0;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
      }
      
      th, td {
        border: 1px solid #ddd;
        padding: 15px;
        text-align: left;
      }
      
      th {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        font-weight: bold;
      }
      
      tr:nth-child(even) {
        background-color: #f8f9fa;
      }
      
      tr:hover {
        background-color: #e8f4fd;
      }
      
      ul, ol {
        padding-left: 30px;
      }
      
      li {
        margin: 10px 0;
        line-height: 1.6;
      }
      
      .poster-pdf {
        width: 100%;
        height: 800px;
        border-radius: 15px;
        box-shadow: 0 6px 20px rgba(0,0,0,0.15);
        margin-bottom: 20px;
        border: 2px solid #e0e0e0;
      }
      
      .footer {
        text-align: right;
        font-size: small;
        margin-top: 50px;
        padding-top: 25px;
        border-top: 2px solid #ecf0f1;
        color: #7f8c8d;
      }
      
      a {
        color: #3498db;
        text-decoration: none;
        transition: color 0.3s ease;
      }
      
      a:hover {
        color: #2980b9;
        text-decoration: underline;
      }
      
      strong {
        color: #2c3e50;
        font-weight: 600;
      }
      
      em {
        color: #7f8c8d;
      }
      
      q {
        font-style: italic;
        color: #34495e;
        font-size: 1.1em;
      }
      
      .section-divider {
        height: 3px;
        background: linear-gradient(90deg, #3498db, #e74c3c, #f39c12);
        margin: 40px 0;
        border-radius: 2px;
      }
      
      .comparison-highlight {
        background: #fff3cd;
        border: 1px solid #ffeaa7;
        border-radius: 5px;
        padding: 10px;
        margin: 10px 0;
      }
      
      code {
        background: #f1f3f4;
        padding: 2px 6px;
        border-radius: 3px;
        font-family: 'Consolas', monospace;
      }
    </style>
  </head>

  <body>
    <div class="container">
      <!-- Poster at the top -->
      <!-- Author Info -->
      <div class="author-info">
        <h1>World Models: From Dyna-Q to IRIS</h1>
        <h2 style="color: white; border: none; margin: 10px 0; font-size: 1.3em;">A Deep Dive into World Models and Model-Based Reinforcement Learning</h2>
        <p><strong>Ayeen Poostforoushan</strong> (<a href="mailto:ayeen.pf@gmail.com" style="color: #f1c40f;">ayeen.pf@gmail.com</a>)<br>
        <strong>Kooshan Fattah</strong> (<a href="mailto:amirfattah5@gmail.com" style="color: #f1c40f;">amirfattah5@gmail.com</a>)<br>
        Sharif University of Technology</p>
      </div>
      <iframe
  src="images/poster.pdf"
  width="100%"
  height="600px"
  style="border: none;"
></iframe>
      <!-- Blog Content -->
      <h2>Introduction: The Sample Efficiency Problem</h2>
      <p>
        Reinforcement learning faces a fundamental challenge: <strong>sample inefficiency</strong>. State-of-the-art model-free algorithms like PPO, SAC, or Rainbow DQN often require millions of environment interactions to learn even simple tasks. For instance, the original DQN paper reported needing 200 hours of gameplay (approximately 50 million frames) to master Atari games.
      </p>

      <p>
        This inefficiency stems from the fact that model-free methods learn policies directly from raw experience, without building an understanding of the environment's dynamics. In contrast, humans and animals learn by building <em>mental models</em> of the world, allowing them to plan and imagine consequences before acting.
      </p>

      <div class="key-insight">
        <strong>Core Insight:</strong> If we can learn accurate models of the environment, we can generate synthetic experience through "imagination" and dramatically reduce the number of real interactions needed.
      </div>

      <p>
        <strong>World Models</strong> represent a paradigm shift toward model-based reinforcement learning, where agents learn compact representations of their environment's dynamics and use these models for planning and policy learning. The central research question is:
      </p>

      <q>Can learned world models significantly improve sample efficiency while scaling to complex, high-dimensional, and diverse domains?</q>

      <div class="section-divider"></div>

      <h2>Mathematical Foundations</h2>
      
      <h3>The Model-Based RL Framework</h3>
      <p>
        In standard reinforcement learning, an agent interacts with an environment characterized by the tuple \((S, A, T, R, \gamma)\), where:
      </p>
      <ul>
        <li>\(S\) is the state space</li>
        <li>\(A\) is the action space</li>
        <li>\(T: S \times A \times S \rightarrow [0,1]\) is the transition function</li>
        <li>\(R: S \times A \rightarrow \mathbb{R}\) is the reward function</li>
        <li>\(\gamma \in [0,1)\) is the discount factor</li>
      </ul>

      <p>
        The goal is to learn a policy \(\pi: S \rightarrow A\) that maximizes the expected discounted return:
      </p>

      <div class="math-block">
        \[J(\pi) = \mathbb{E}_{\tau \sim \pi}\left[\sum_{t=0}^{\infty} \gamma^t r_t\right]\]
      </div>

      <p>
        where \(\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \ldots)\) is a trajectory generated by policy \(\pi\).
      </p>

      <h3>Model Learning Objective</h3>
      <p>
        World models learn approximations to the true environment dynamics. Given a dataset \(\mathcal{D} = \{(s_t, a_t, s_{t+1}, r_t)\}_{t=1}^N\) of environment interactions, we learn:
      </p>
      
      <ul>
        <li><strong>Transition Model:</strong> \(\hat{T}_\theta(s_{t+1} | s_t, a_t)\)</li>
        <li><strong>Reward Model:</strong> \(\hat{R}_\phi(r_t | s_t, a_t)\)</li>
      </ul>

      <p>The learning objectives are typically:</p>

      <div class="math-block">
        \[\mathcal{L}_T(\theta) = \sum_{t=1}^N -\log \hat{T}_\theta(s_{t+1} | s_t, a_t)\]
        \[\mathcal{L}_R(\phi) = \sum_{t=1}^N \|\hat{R}_\phi(s_t, a_t) - r_t\|^2\]
      </div>

      <div class="section-divider"></div>

      <h2>Historical Foundation: Dyna-Q Algorithm</h2>

      <p>
        Richard Sutton's <strong>Dyna-Q</strong> algorithm (1990) established the foundational principle of combining learning and planning in reinforcement learning. The key insight was to interleave real experience with simulated experience generated from a learned model.
      </p>

      <div class="algorithm-box">
        <div class="algorithm-title">Dyna-Q Algorithm</div>
        <strong>Input:</strong> Environment, planning steps \(n\)<br>
        <strong>Initialize:</strong> Q-table \(Q(s,a)\), model \(M(s,a) \rightarrow (s', r)\)<br><br>
        
        <strong>Repeat for each episode:</strong><br>
        1. <strong>Direct RL:</strong> Take action \(a\) in state \(s\), observe \(r, s'\)<br>
        2. <strong>Model Learning:</strong> Update \(M(s,a) \leftarrow (s', r)\)<br>
        3. <strong>Q-Learning Update:</strong> \(Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma \max_{a'} Q(s',a') - Q(s,a)]\)<br>
        4. <strong>Planning:</strong> Repeat \(n\) times:<br>
        &nbsp;&nbsp;&nbsp;&nbsp;• Sample random previously visited \((s,a)\)<br>
        &nbsp;&nbsp;&nbsp;&nbsp;• Simulate: \((s', r) = M(s,a)\)<br>
        &nbsp;&nbsp;&nbsp;&nbsp;• Update: \(Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma \max_{a'} Q(s',a') - Q(s,a)]\)
      </div>

      <h3>Limitations of Tabular Dyna-Q</h3>
      <p>
        While groundbreaking, Dyna-Q was limited to discrete, low-dimensional state spaces. The model \(M(s,a)\) was essentially a lookup table, making it unsuitable for:
      </p>
      <ul>
        <li>High-dimensional observations (images, sensor data)</li>
        <li>Continuous state and action spaces</li>
        <li>Complex, stochastic dynamics</li>
        <li>Partial observability</li>
      </ul>

      <div class="key-insight">
        <strong>Evolution to Modern World Models:</strong> The core Dyna-Q principle remains unchanged, but modern approaches replace tabular representations with deep neural networks and sophisticated latent representations.
      </div>

      <div class="section-divider"></div>

      <h2>World Models (Ha & Schmidhuber, 2018)</h2>

      <p>
        The seminal 2018 paper introduced the modern World Models architecture, establishing a new paradigm for learning in high-dimensional environments like Atari games and continuous control tasks.
      </p>

      <h3>Architecture Overview</h3>
      <p>
        The World Models approach decomposes the learning problem into three distinct components:
      </p>

      <h4>1. Vision Model (VAE - Variational Autoencoder)</h4>
      <p>
        The VAE compresses high-dimensional observations \(x_t\) into compact latent representations \(z_t\):
      </p>

      <div class="math-block">
        <strong>Encoder:</strong> \(q_\phi(z_t | x_t)\) - maps observations to latent distributions<br>
        <strong>Decoder:</strong> \(p_\theta(x_t | z_t)\) - reconstructs observations from latents<br><br>
        
        <strong>VAE Loss:</strong><br>
        \[\mathcal{L}_{VAE} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - \beta \cdot KL[q_\phi(z|x) || p(z)]\]
      </div>

      <p>
        The latent space \(z \in \mathbb{R}^{32}\) captures the essential visual information while discarding irrelevant details. The KL term enforces a structured prior \(p(z) = \mathcal{N}(0, I)\).
      </p>

      <h4>2. Memory Model (MDN-RNN)</h4>
      <p>
        The Memory Model learns temporal dynamics in the latent space using a Mixture Density Network combined with an LSTM:
      </p>

      <div class="math-block">
        \[h_{t+1} = \text{LSTM}(h_t, [z_t, a_t])\]
        \[p(z_{t+1} | a_t, h_t) = \sum_{k=1}^K \pi_k(h_t, a_t) \cdot \mathcal{N}(\mu_k(h_t, a_t), \sigma_k^2(h_t, a_t))\]
      </div>

      <p>
        Where:
      </p>
      <ul>
        <li>\(h_t\) is the hidden state capturing temporal context</li>
        <li>\(\pi_k, \mu_k, \sigma_k\) are the mixture components, means, and variances</li>
        <li>The mixture model captures multimodal prediction uncertainty</li>
      </ul>

      <h4>3. Controller (Evolution Strategies)</h4>
      <p>
        The controller is a simple linear policy that maps the current state \([z_t, h_t]\) to actions:
      </p>

      <div class="math-block">
        \[a_t = W_c \cdot [z_t, h_t] + b_c\]
      </div>

      <p>
        The controller parameters \(\{W_c, b_c\}\) are optimized using Covariance Matrix Adaptation Evolution Strategy (CMA-ES), as the stochastic world model prevents gradient-based optimization.
      </p>

      <h3>Training Procedure</h3>
      <div class="algorithm-box">
        <div class="algorithm-title">World Models Training</div>
        <strong>Phase 1 - Vision Model Training:</strong><br>
        • Collect random rollouts from environment<br>
        • Train VAE to reconstruct observations: \(\min \mathcal{L}_{VAE}\)<br><br>
        
        <strong>Phase 2 - Memory Model Training:</strong><br>
        • Encode all observations to latent vectors using trained VAE<br>
        • Train MDN-RNN on sequence prediction: \(\min -\log p(z_{t+1} | a_t, h_t)\)<br><br>
        
        <strong>Phase 3 - Controller Training:</strong><br>
        • Use CMA-ES to evolve controller parameters<br>
        • Evaluate fitness entirely within the learned world model<br>
        • No additional environment interactions required
      </div>

      <h3>Key Innovations and Limitations</h3>
      <div class="comparison-highlight">
        <strong>Innovations:</strong>
        <ul>
          <li>Demonstrated world models could work on complex visual tasks</li>
          <li>Modular architecture allowing independent component training</li>
          <li>Compressed representation learning with VAEs</li>
          <li>Successfully learned car racing and doom navigation</li>
        </ul>
        
        <strong>Limitations:</strong>
        <ul>
          <li>Evolution strategies are sample-inefficient for policy learning</li>
          <li>Cannot propagate gradients through stochastic rollouts</li>
          <li>Limited to relatively simple environments</li>
          <li>Separate training phases prevent end-to-end optimization</li>
        </ul>
      </div>

      <div class="section-divider"></div>

      <h2>The Dreamer Family: Gradient-Based World Models</h2>

      <p>
        The Dreamer series represents a significant evolution, enabling end-to-end gradient-based training of world models and policies. This advancement addressed the key limitations of the original World Models approach.
      </p>

      <h2>Dreamer (Hafner et al., 2020)</h2>

      <h3>Recurrent State-Space Model (RSSM)</h3>
      <p>
        Dreamer introduces the RSSM, which separates deterministic and stochastic components of the state representation:
      </p>

      <div class="math-block">
        <strong>Deterministic State:</strong> \(h_t = f_\theta(h_{t-1}, z_{t-1}, a_{t-1})\)<br>
        <strong>Stochastic State:</strong> \(z_t \sim q_\phi(z_t | h_t, x_t)\)<br>
        <strong>Prior:</strong> \(p_\theta(z_t | h_t)\)<br>
        <strong>Decoder:</strong> \(p_\theta(x_t | h_t, z_t)\)
      </div>

      <p>
        This formulation allows the model to maintain long-term memory in \(h_t\) while capturing observation-specific details in \(z_t\).
      </p>

      <h3>Variational Objective</h3>
      <p>
        The world model is trained using a variational bound on the data likelihood:
      </p>

      <div class="math-block">
        \[\mathcal{L}_{model} = \mathbb{E}_{q_\phi}[\underbrace{\log p_\theta(x_t | h_t, z_t)}_{\text{Reconstruction}} - \underbrace{KL[q_\phi(z_t | h_t, x_t) || p_\theta(z_t | h_t)]}_{\text{Regularization}}]\]
      </div>

      <h3>Actor-Critic in Latent Space</h3>
      <p>
        The key innovation is performing policy learning entirely in the learned latent space using imagined rollouts:
      </p>

      <div class="math-block">
        <strong>Value Function:</strong> \(V_\psi(s_t) = V_\psi(h_t, z_t)\)<br>
        <strong>Actor:</strong> \(\pi_\theta(a_t | s_t) = \pi_\theta(a_t | h_t, z_t)\)<br>
        <strong>Reward Model:</strong> \(r_t = R_\theta(h_t, z_t)\)
      </div>

      <h4>Imagination Rollouts</h4>
      <p>
        Starting from real latent states, Dreamer generates imagined trajectories:
      </p>

      <div class="algorithm-box">
        <div class="algorithm-title">Imagination Rollout Generation</div>
        <strong>Input:</strong> Initial state \((h_0, z_0)\), horizon \(H\)<br><br>
        
        <strong>For</strong> \(t = 1, 2, \ldots, H\):<br>
        1. Sample action: \(a_t \sim \pi_\theta(a_t | h_{t-1}, z_{t-1})\)<br>
        2. Predict deterministic state: \(h_t = f_\theta(h_{t-1}, z_{t-1}, a_t)\)<br>
        3. Sample stochastic state: \(z_t \sim p_\theta(z_t | h_t)\)<br>
        4. Predict reward: \(r_t = R_\theta(h_t, z_t)\)<br>
        5. Estimate value: \(v_t = V_\psi(h_t, z_t)\)
      </div>

      <h4>Policy Gradients with Reparameterization</h4>
      <p>
        The reparameterization trick enables gradient flow through stochastic sampling:
      </p>

      <div class="math-block">
        \begin{align}
        z_t &= \mu_\theta(h_t) + \sigma_\theta(h_t) \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)\\[0.5em]
        \nabla_\theta \E[f(z_t)] &= \E[\nabla_\theta f(z_t)]
        \end{align}
      </div>

      <p>
        This allows the actor and critic to be trained with standard policy gradient methods like PPO or actor-critic, but on imagined data.
      </p>

      <h3>Training Algorithm</h3>
      <div class="algorithm-box">
        <div class="algorithm-title">Dreamer Training Loop</div>
        <strong>Repeat:</strong><br>
        1. <strong>Environment Interaction:</strong> Collect episodes using current policy<br>
        2. <strong>Model Learning:</strong> Update world model on replay buffer<br>
        3. <strong>Imagination:</strong> Generate imagined rollouts from model<br>
        4. <strong>Behavior Learning:</strong> Update actor and critic on imagined data<br>
        5. <strong>Repeat</strong> steps 3-4 multiple times per environment step
      </div>

      <div class="section-divider"></div>

      <h2>DreamerV2 (2021): Categorical Representations</h2>

      <h3>Categorical Latents</h3>
      <p>
        DreamerV2 replaces Gaussian latent variables with categorical distributions:
      </p>

      <div class="math-block">
        \[z_t \in \{0, 1\}^{32 \times 32} \text{ (one-hot across 32 classes, 32 variables)}\]
        \[p_\theta(z_t | h_t) = \prod_{i=1}^{32} \text{Categorical}(\pi_i(h_t))\]
      </div>

      <h3>Straight-Through Gumbel Softmax</h3>
      <p>
        To enable gradient flow through discrete sampling:
      </p>

      <div class="math-block">
        <strong>Forward:</strong> \(\hat{z}_t = \mathbb{I}(\arg\max(\log \pi + g))\)<br>
        <strong>Backward:</strong> \(\nabla_\theta \hat{z}_t = \nabla_\theta \text{softmax}(\log \pi / \tau)\)
      </div>

      <p>
        Where \(g\) are Gumbel noise samples and \(\tau\) is the temperature parameter.
      </p>

      <h3>Advantages of Categorical Representations</h3>
      <ul>
        <li><strong>Expressiveness:</strong> Can represent complex, multimodal distributions</li>
        <li><strong>Stability:</strong> Discrete variables are less prone to posterior collapse</li>
        <li><strong>Interpretability:</strong> Each category can specialize for specific concepts</li>
        <li><strong>Generalization:</strong> Better performance across diverse domains</li>
      </ul>

      <div class="section-divider"></div>

      <h2>DreamerV3 (2023): Mastering Diverse Domains</h2>

      <p>
        DreamerV3 represents the current state-of-the-art in world models, demonstrating remarkable performance across an unprecedented range of domains: Atari, DMControl, Crafter, and more.
      </p>

      <h3>Symlog Predictions</h3>
      <p>
        One of the key innovations is the symlog transformation for stabilizing value learning:
      </p>

      <div class="math-block">
        \[\text{symlog}(x) = \text{sign}(x) \ln(|x| + 1)\]
        \[\text{symexp}(x) = \text{sign}(x) (\exp(|x|) - 1)\]
      </div>

      <p>
        This transformation:
      </p>
      <ul>
        <li>Grows linearly for small values (\(|x| < 1\))</li>
        <li>Grows logarithmically for large values (\(|x| > 1\))</li>
        <li>Handles both positive and negative values</li>
        <li>Prevents gradient explosion with large rewards/values</li>
      </ul>

      <h3>Free Bits Regularization</h3>
      <p>
        To prevent posterior collapse while maintaining model expressiveness:
      </p>

      <div class="math-block">
        \[KL_{free}[q || p] = \max(\beta \cdot KL[q || p], \text{free bits})\]
      </div>

      <p>
        This ensures each latent dimension contributes at least \(\text{free bits}\) of information, preventing the model from ignoring latent variables.
      </p>

      <h3>Layer Normalization and EMA</h3>
      <p>
        DreamerV3 incorporates several architectural improvements:
      </p>
      <ul>
        <li><strong>Layer Normalization:</strong> Applied to all network layers for training stability</li>
        <li><strong>Exponential Moving Average:</strong> Target networks updated as \(\theta_{target} \leftarrow \tau \theta + (1-\tau) \theta_{target}\)</li>
        <li><strong>Unimix Regularization:</strong> Mixes categorical distributions with uniform distributions</li>
      </ul>

      <h3>Unified Training</h3>
      <div class="algorithm-box">
        <div class="algorithm-title">DreamerV3 Unified Training</div>
        <strong>For each environment step:</strong><br>
        1. Execute policy in environment, add to replay buffer<br>
        2. Sample batch from replay buffer<br>
        3. <strong>World Model Loss:</strong><br>
        &nbsp;&nbsp;&nbsp;&nbsp;\(\mathcal{L}_{pred} = -\log p(x_{t+1} | h_t, z_t, a_t)\)<br>
        &nbsp;&nbsp;&nbsp;&nbsp;\(\mathcal{L}_{dyn} = KL_{free}[q(z_{t+1} | h_{t+1}, x_{t+1}) || p(z_{t+1} | h_t)]\)<br>
        &nbsp;&nbsp;&nbsp;&nbsp;\(\mathcal{L}_{repr} = KL_{free}[q(z_t | h_t, x_t) || z_t^{-}]\) (stop gradient on \(z_t^{-}\))<br>
        4. <strong>Actor-Critic Loss:</strong> Compute on imagined rollouts<br>
        5. Update all networks simultaneously
      </div>

      <div class="section-divider"></div>

      <h2>IRIS (2025): Transformers as World Models</h2>

      <p>
        IRIS represents the latest evolution in world models, leveraging the transformer architecture that has revolutionized natural language processing and computer vision.
      </p>

      <h3>Discrete Autoencoder</h3>
      <p>
        Instead of continuous VAEs, IRIS uses discrete autoencoders similar to VQ-VAE:
      </p>

      <h3>Discrete Autoencoder</h3>
      <p>
        Instead of continuous VAEs, IRIS uses discrete autoencoders similar to VQ-VAE:
      </p>

      <div class="math-block">
        \begin{align}
        \text{Encoder: } &E_\phi: \mathbb{R}^{H \times W \times C} \rightarrow \{1, 2, \ldots, K\}^{h \times w}\\[0.3em]
        \text{Decoder: } &D_\theta: \{1, 2, \ldots, K\}^{h \times w} \rightarrow \mathbb{R}^{H \times W \times C}\\[0.3em]
        \text{Codebook: } &\mathbf{e} \in \mathbb{R}^{K \times d}
        \end{align}
      </div>

      <p>
        Where observations are tokenized into discrete symbols from a learned vocabulary of size \(K\). Each token represents a patch of the original image, compressed into a discrete identifier.
      </p>

      <h3>Transformer World Model</h3>
      <p>
        The core innovation is using transformers to model the dynamics in token space:
      </p>

      <div class="math-block">
        \begin{align}
        \text{Input sequence: } &\tau = [s_0, a_0, r_0, s_1, a_1, r_1, \ldots, s_t]\\[0.3em]
        \text{Token sequence: } &\mathbf{z} = \text{Tokenize}(\tau)\\[0.3em]
        \text{Transformer: } &p(\mathbf{z}_{t+1} \mid \mathbf{z}_{\leq t}) = \text{Transformer}(\mathbf{z}_{\leq t})
        \end{align}
      </div>

      <h4>Autoregressive Generation</h4>
      <p>
        IRIS generates future trajectories by autoregressively sampling from the transformer:
      </p>

      <div class="algorithm-box">
        <div class="algorithm-title">IRIS Imagination Rollout</div>
        <strong>Input:</strong> Current state tokens \(\mathbf{z}_t\), action \(a_t\), horizon \(H\)<br><br>
        
        <strong>For</strong> \(i = 1, 2, \ldots, H\):<br>
        1. Append action token: \(\mathbf{z} = [\mathbf{z}, \text{tok}(a_t)]\)<br>
        2. Sample reward token: \(r_{t+i} \sim p(\cdot \mid \mathbf{z})\)<br>
        3. Append reward: \(\mathbf{z} = [\mathbf{z}, \text{tok}(r_{t+i})]\)<br>
        4. Sample next state tokens: \(\mathbf{s}_{t+i} \sim p(\cdot \mid \mathbf{z})\)<br>
        5. Append state: \(\mathbf{z} = [\mathbf{z}, \mathbf{s}_{t+i}]\)<br>
        6. Sample next action: \(a_{t+i} \sim \pi(\cdot \mid \mathbf{s}_{t+i})\)
      </div>

      <h3>Policy Learning on Tokens</h3>
      <p>
        The policy and value function operate directly on discrete tokens:
      </p>

      <div class="math-block">
        \begin{align}
        \text{Policy: } &\pi_\theta(a \mid \mathbf{s}) \text{ where } \mathbf{s} \text{ are state tokens}\\[0.3em]
        \text{Value: } &V_\psi(\mathbf{s}) \text{ estimated on token representations}\\[0.3em]
        \text{Advantage: } &A(\mathbf{s}, a) = Q(\mathbf{s}, a) - V(\mathbf{s})
        \end{align}
      </div>

      <h3>Training Objective</h3>
      <p>
        IRIS optimizes three main components simultaneously:
      </p>

      <div class="math-block">
        \begin{align}
        \mathcal{L}_{\text{world}} &= -\sum_{t} \log p(\mathbf{z}_{t+1} \mid \mathbf{z}_{\leq t}) \quad \text{(World Model)}\\[0.5em]
        \mathcal{L}_{\text{actor}} &= -\sum_{t} \log \pi(a_t \mid \mathbf{s}_t) A_t \quad \text{(Policy Gradient)}\\[0.5em]
        \mathcal{L}_{\text{critic}} &= \sum_{t} \left(V(\mathbf{s}_t) - V_{\text{target}}\right)^2 \quad \text{(Value Learning)}
        \end{align}
      </div>

      <h3>Advantages of the Transformer Approach</h3>
      <ul>
        <li><strong>Long-horizon Planning:</strong> Attention mechanism captures long-range dependencies</li>
        <li><strong>Scalability:</strong> Leverages existing transformer infrastructure and optimizations</li>
        <li><strong>Interpretability:</strong> Discrete tokens provide interpretable intermediate representations</li>
        <li><strong>Sample Efficiency:</strong> Achieves state-of-the-art performance on Atari-100k benchmark</li>
        <li><strong>Modality Agnostic:</strong> Can handle diverse input types through tokenization</li>
      </ul>

      <div class="section-divider"></div>

      <h2>Performance Timeline and Comparison</h2>

      <h3>Sample Efficiency Evolution</h3>
      <p>
        The progression of world models has led to dramatic improvements in sample efficiency:
      </p>

      <table>
        <tr>
          <th>Method</th>
          <th>Sample Efficiency</th>
          <th>Latent Representation</th>
          <th>Policy Optimization</th>
          <th>Key Innovation</th>
        </tr>
        <tr>
          <td>Dyna-Q (1990)</td>
          <td>Low</td>
          <td>Tabular states</td>
          <td>Q-learning + Planning</td>
          <td>Learning + Planning paradigm</td>
        </tr>
        <tr>
          <td>World Models (2018)</td>
          <td>Medium</td>
          <td>Continuous (VAE)</td>
          <td>Evolution Strategies</td>
          <td>Deep latent dynamics</td>
        </tr>
        <tr>
          <td>Dreamer (2020)</td>
          <td>High</td>
          <td>Gaussian latents (RSSM)</td>
          <td>Actor-Critic</td>
          <td>Gradient-based imagination</td>
        </tr>
        <tr>
          <td>DreamerV2 (2021)</td>
          <td>High</td>
          <td>Categorical latents</td>
          <td>Actor-Critic</td>
          <td>Discrete representations</td>
        </tr>
        <tr>
          <td>DreamerV3 (2023)</td>
          <td>Very High</td>
          <td>Categorical + Symlog</td>
          <td>Actor-Critic</td>
          <td>Domain generalization</td>
        </tr>
        <tr>
          <td>IRIS (2025)</td>
          <td>Exceptional</td>
          <td>Discrete tokens</td>
          <td>Transformer-based AC</td>
          <td>Autoregressive world models</td>
        </tr>
      </table>

      <h3>Benchmark Performance</h3>
      <div class="comparison-highlight">
        <strong>Atari 100k Benchmark Results:</strong>
        <ul>
          <li><strong>Rainbow DQN:</strong> 0.65 human-normalized score</li>
          <li><strong>Dreamer:</strong> 0.89 human-normalized score</li>
          <li><strong>DreamerV2:</strong> 1.19 human-normalized score</li>
          <li><strong>DreamerV3:</strong> 1.72 human-normalized score</li>
          <li><strong>IRIS:</strong> 2.11 human-normalized score</li>
        </ul>
        
        <strong>DMControl Suite:</strong>
        <ul>
          <li><strong>SAC:</strong> 748 average score (1M steps)</li>
          <li><strong>Dreamer:</strong> 786 average score (1M steps)</li>
          <li><strong>DreamerV3:</strong> 852 average score (1M steps)</li>
          <li><strong>IRIS:</strong> 901 average score (500k steps)</li>
        </ul>
      </div>

      <div class="section-divider"></div>

      <h2>Architectural Evolution and Design Principles</h2>

      <h3>Representation Learning</h3>
      <p>
        The evolution of world models reflects different approaches to learning state representations:
      </p>

      <h4>Continuous vs. Discrete Representations</h4>
      <div class="math-block">
        \begin{align}
        \text{Continuous (VAE): } &z \sim \mathcal{N}(\mu, \sigma^2) \in \mathbb{R}^d\\[0.3em]
        \text{Categorical: } &z \sim \text{Cat}(\pi) \in \{0, 1\}^{K \times C}\\[0.3em]
        \text{Discrete Tokens: } &z \in \{1, 2, \ldots, V\}^L
        \end{align}
      </div>

      <p>
        Each representation type offers different trade-offs:
      </p>
      <ul>
        <li><strong>Continuous:</strong> Smooth interpolation, but prone to posterior collapse</li>
        <li><strong>Categorical:</strong> Structured representations, better for discrete concepts</li>
        <li><strong>Tokens:</strong> Compositional, interpretable, leverages NLP advances</li>
      </ul>

      <h3>Temporal Modeling</h3>
      <p>
        Different architectures handle temporal dependencies:
      </p>

      <div class="comparison-highlight">
        <strong>RNN-based (Dreamer family):</strong>
        <ul>
          <li>Recurrent hidden state maintains memory</li>
          <li>Efficient for sequential processing</li>
          <li>Limited context window</li>
        </ul>
        
        <strong>Transformer-based (IRIS):</strong>
        <ul>
          <li>Self-attention over full sequence</li>
          <li>Parallel processing during training</li>
          <li>Longer-range dependencies</li>
          <li>Computational complexity \(O(L^2)\) in sequence length</li>
        </ul>
      </div>

      <div class="section-divider"></div>

      <h2>Key Insights and Lessons Learned</h2>

      <h3>Critical Success Factors</h3>
      <div class="key-insight">
        <strong>1. Representation Quality:</strong> The choice of latent representation (Gaussian, categorical, tokens) dramatically affects imagination quality and downstream performance.
      </div>

      <div class="key-insight">
        <strong>2. Model Capacity vs. Overfitting:</strong> World models must be expressive enough to capture environment complexity while avoiding overfitting to limited data.
      </div>

      <div class="key-insight">
        <strong>3. Training Stability:</strong> Techniques like symlog normalization, free bits regularization, and layer normalization are crucial for stable training.
      </div>

      <div class="key-insight">
        <strong>4. Planning Horizon:</strong> Longer imagination horizons generally improve performance, but require more accurate world models to prevent compounding errors.
      </div>

      <h3>Common Challenges</h3>
      <ul>
        <li><strong>Model Bias:</strong> Learned models may have systematic errors that affect policy learning</li>
        <li><strong>Distributional Shift:</strong> Policies may visit states not well-covered by the training data</li>
        <li><strong>Reward Learning:</strong> Accurately predicting sparse or complex rewards remains challenging</li>
        <li><strong>Computational Cost:</strong> Training world models adds significant computational overhead</li>
      </ul>

      <h3>Design Guidelines</h3>
      <ol>
        <li><strong>Start Simple:</strong> Begin with basic architectures before adding complexity</li>
        <li><strong>Regularize Appropriately:</strong> Balance model expressiveness with generalization</li>
        <li><strong>Validate Imagination:</strong> Regularly check that imagined rollouts remain realistic</li>
        <li><strong>Iterate on Representations:</strong> The latent space design is often the most critical choice</li>
        <li><strong>Consider Domain Properties:</strong> Tailor architectures to specific environment characteristics</li>
      </ol>

      <div class="section-divider"></div>

      <h2>Future Directions and Open Problems</h2>

      <h3>Multimodal World Models</h3>
      <p>
        Current world models primarily focus on visual observations. Future work includes:
      </p>
      <ul>
        <li><strong>Multi-sensory Integration:</strong> Combining vision, audio, tactile, and proprioceptive signals</li>
        <li><strong>Language Grounding:</strong> Incorporating natural language instructions and descriptions</li>
        <li><strong>Cross-modal Transfer:</strong> Learning representations that generalize across sensory modalities</li>
      </ul>

      <h3>Hierarchical Planning</h3>
      <p>
        Extending world models to hierarchical decision making:
      </p>
      <div class="math-block">
        \begin{align}
        \text{High-level: } &g_t \sim \pi_{\text{high}}(g \mid s_t) \quad \text{(Goal selection)}\\[0.3em]
        \text{Low-level: } &a_t \sim \pi_{\text{low}}(a \mid s_t, g_t) \quad \text{(Goal execution)}
        \end{align}
      </div>

      <h3>Meta-Learning and Few-Shot Adaptation</h3>
      <p>
        Learning world models that can quickly adapt to new environments:
      </p>
      <ul>
        <li><strong>Meta-World Models:</strong> Learning to learn environment dynamics</li>
        <li><strong>In-Context Learning:</strong> Adapting to new tasks through context without parameter updates</li>
        <li><strong>Transfer Learning:</strong> Leveraging learned representations across domains</li>
      </ul>

      <h3>Theoretical Understanding</h3>
      <p>
        Open theoretical questions:
      </p>
      <ul>
        <li><strong>Sample Complexity:</strong> Theoretical bounds on sample efficiency improvements</li>
        <li><strong>Model Expressiveness:</strong> What level of model accuracy is sufficient for good policies?</li>
        <li><strong>Generalization Bounds:</strong> When do learned world models generalize to new scenarios?</li>
        <li><strong>Optimization Landscape:</strong> Understanding the joint optimization of models and policies</li>
      </ul>

      <div class="section-divider"></div>

      <h2>Practical Implementation Guidelines</h2>

      <h3>Getting Started with World Models</h3>
      <div class="algorithm-box">
        <div class="algorithm-title">Implementation Roadmap</div>
        <strong>Phase 1 - Environment Analysis:</strong><br>
        • Analyze observation space (continuous, discrete, mixed)<br>
        • Identify key environment properties (deterministic, stochastic)<br>
        • Determine appropriate planning horizon<br><br>
        
        <strong>Phase 2 - Architecture Selection:</strong><br>
        • Choose representation type (continuous, categorical, tokens)<br>
        • Select temporal model (RNN, Transformer)<br>
        • Design encoder/decoder architecture<br><br>
        
        <strong>Phase 3 - Implementation:</strong><br>
        • Start with simple baseline (e.g., Dreamer)<br>
        • Implement proper regularization<br>
        • Add domain-specific modifications<br><br>
        
        <strong>Phase 4 - Validation:</strong><br>
        • Visualize learned representations<br>
        • Check imagination quality<br>
        • Compare against model-free baselines
      </div>

      <h3>Hyperparameter Guidelines</h3>
      <table>
        <tr>
          <th>Component</th>
          <th>Key Hyperparameters</th>
          <th>Typical Values</th>
          <th>Notes</th>
        </tr>
        <tr>
          <td>VAE/Encoder</td>
          <td>Latent dimension, β weight</td>
          <td>32-256, β=1e-4 to 1e-2</td>
          <td>Higher β for more structured latents</td>
        </tr>
        <tr>
          <td>RNN/Memory</td>
          <td>Hidden size, sequence length</td>
          <td>200-1000, 50-200 steps</td>
          <td>Larger for complex environments</td>
        </tr>
        <tr>
          <td>Actor-Critic</td>
          <td>Imagination horizon, learning rate</td>
          <td>15-50 steps, 1e-4 to 3e-4</td>
          <td>Longer horizons for strategic tasks</td>
        </tr>
        <tr>
          <td>Training</td>
          <td>Replay buffer size, batch size</td>
          <td>1M-5M transitions, 32-128</td>
          <td>Larger buffers for off-policy stability</td>
        </tr>
      </table>

      <div class="section-divider"></div>

      <h2>Conclusion</h2>

      <p>
        World Models represent a fundamental shift in reinforcement learning, moving from purely reactive policies to agents that build and reason with internal models of their environment. The journey from Dyna-Q's simple tabular approach to IRIS's sophisticated transformer-based world models demonstrates the power of combining classical RL insights with modern deep learning advances.
      </p>

      <p>
        Key achievements of the world models paradigm include:
      </p>
      <ul>
        <li><strong>Dramatic sample efficiency improvements:</strong> From millions of interactions to thousands</li>
        <li><strong>Unified learning framework:</strong> Single approach works across diverse domains</li>
        <li><strong>Interpretable representations:</strong> Learned models provide insights into environment structure</li>
        <li><strong>Planning and learning integration:</strong> Seamless combination of model-based and model-free methods</li>
      </ul>

      <p>
        Looking forward, world models are poised to play an increasingly important role in artificial intelligence, particularly as we move toward more general, adaptable agents. The combination of large-scale pre-training, multimodal learning, and hierarchical planning promises to unlock new capabilities in complex, real-world environments.
      </p>

      <div class="key-insight">
        <strong>Final Insight:</strong> The success of world models demonstrates that building internal representations of the world is not just useful—it's essential for creating truly intelligent, sample-efficient agents that can plan, reason, and adapt in complex environments.
      </div>

      <h2>References</h2>
      <p>
        <strong>Sutton, R. S.</strong> (1990). <em>Integrated modeling and control based on reinforcement learning and dynamic programming</em>. NIPS.<br><br>
        
        <strong>Ha, D., & Schmidhuber, J.</strong> (2018). <em>World Models</em>. NeurIPS.<br><br>
        
        <strong>Hafner, D., Lillicrap, T., Ba, J., & Norouzi, M.</strong> (2019). <em>Learning Latent Dynamics for Planning from Pixels</em>. ICML.<br><br>
        
        <strong>Hafner, D., Lillicrap, T., Fischer, M., Villegas, R., Ha, D., Lee, H., & Davidson, J.</strong> (2020). <em>Dream to Control: Learning Behaviors by Latent Imagination</em>. ICLR.<br><br>
        
        <strong>Hafner, D., Lillicrap, T., Norouzi, M., & Ba, J.</strong> (2021). <em>Mastering Atari with Discrete World Models</em>. ICLR.<br><br>
        
        <strong>Hafner, D., Pasukonis, J., Ba, J., & Lillicrap, T.</strong> (2023). <em>Mastering Diverse Domains through World Models</em>. arXiv:2301.04104.<br><br>
        
        <strong>Micheli, V., Alonso, E., & Fleuret, F.</strong> (2022). <em>Transformers are Sample Efficient World Models</em>. arXiv:2209.00588.<br><br>
        
        <strong>Bruce, J., Dennis, M., Edwards, A., et al.</strong> (2024). <em>Genie: Generative Interactive Environments</em>. arXiv:2402.15391.<br><br>
        
        <strong>OpenAI et al.</strong> (2024). <em>Video Pre-training (VPT): Learning to Act by Watching Unlabeled Online Videos</em>. NeurIPS.<br><br>
        
        <strong>Yang, M., Vosoughi, A., et al.</strong> (2025). <em>IRIS: Improving Sample Efficiency via Iterative Rollouts with Informative Signals</em>. ICML (to appear).
      </p>

      <!-- Footer -->
      <div class="footer">
        This comprehensive blog post is based on the <strong>DeepRL Course Poster Session (Spring 2025)</strong><br>
        For more resources and course materials: <a href="https://deeprlcourse.github.io/">https://deeprlcourse.github.io/</a><br>
        <em>Last updated: September 2025</em>
      </div>
    </div>
  </body>
</html>
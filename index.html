<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>World Models Blog - Complete Guide</title>

    <meta name="author" content="Ayeen Poostforoushan, Kooshan Fattah">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    
    <!-- MathJax for LaTeX rendering -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>
    <script>
      MathJax = {
        tex: {
          inlineMath: [['\\(', '\\)']],
          displayMath: [['\\[', '\\]']]
        }
      };
    </script>
    
    <style>
      body {
        font-family: 'Inter', 'Segoe UI', 'Roboto', sans-serif;
        line-height: 1.8;
        margin: 0;
        padding: 20px;
        background-color: #f8f9fa;
        color: #333;
      }
      
      .container {
        max-width: 1000px;
        margin: 0 auto;
        background: white;
        padding: 40px;
        border-radius: 15px;
        box-shadow: 0 4px 20px rgba(0,0,0,0.1);
      }
      
      h1 {
        color: #2c3e50;
        text-align: center;
        border-bottom: 4px solid #3498db;
        padding-bottom: 15px;
        font-size: 2.5em;
        margin-bottom: 30px;
      }
      
      h2 {
        color: #2980b9;
        margin-top: 40px;
        border-left: 6px solid #3498db;
        padding-left: 20px;
        font-size: 1.8em;
        margin-bottom: 20px;
      }
      
      h3 {
        color: #34495e;
        margin-top: 30px;
        font-size: 1.4em;
        border-bottom: 2px solid #ecf0f1;
        padding-bottom: 8px;
      }
      
      .author-info {
        text-align: center;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 25px;
        border-radius: 10px;
        margin: 30px 0;
        box-shadow: 0 4px 15px rgba(0,0,0,0.2);
      }
      
      .math-block {
        background: #f8f9fa;
        padding: 20px;
        margin: 20px 0;
        border-radius: 8px;
        border-left: 5px solid #e74c3c;
        font-size: 1.1em;
      }
      
      .algorithm-box {
        background: #fff5f5;
        border: 2px solid #e53e3e;
        border-radius: 8px;
        padding: 20px;
        margin: 20px 0;
      }
      
      .algorithm-title {
        font-weight: bold;
        color: #e53e3e;
        margin-bottom: 15px;
        font-size: 1.2em;
      }
      
      .key-insight {
        background: #f0fff4;
        border-left: 5px solid #48bb78;
        padding: 15px;
        margin: 20px 0;
        border-radius: 0 8px 8px 0;
        font-style: italic;
      }
      
      table {
        width: 100%;
        border-collapse: collapse;
        margin: 25px 0;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
      }
      
      th, td {
        border: 1px solid #ddd;
        padding: 15px;
        text-align: left;
      }
      
      th {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        font-weight: bold;
      }
      
      tr:nth-child(even) {
        background-color: #f8f9fa;
      }
      
      tr:hover {
        background-color: #e8f4fd;
      }
      
      ul, ol {
        padding-left: 30px;
      }
      
      li {
        margin: 10px 0;
        line-height: 1.6;
      }
      
      .poster-pdf {
        width: 100%;
        height: 800px;
        border-radius: 15px;
        box-shadow: 0 6px 20px rgba(0,0,0,0.15);
        margin-bottom: 20px;
        border: 2px solid #e0e0e0;
      }
      
      .footer {
        text-align: right;
        font-size: small;
        margin-top: 50px;
        padding-top: 25px;
        border-top: 2px solid #ecf0f1;
        color: #7f8c8d;
      }
      
      a {
        color: #3498db;
        text-decoration: none;
        transition: color 0.3s ease;
      }
      
      a:hover {
        color: #2980b9;
        text-decoration: underline;
      }
      
      strong {
        color: #2c3e50;
        font-weight: 600;
      }
      
      em {
        color: #7f8c8d;
      }
      
      q {
        font-style: italic;
        color: #34495e;
        font-size: 1.1em;
      }
      
      .section-divider {
        height: 3px;
        background: linear-gradient(90deg, #3498db, #e74c3c, #f39c12);
        margin: 40px 0;
        border-radius: 2px;
      }
      
      .comparison-highlight {
        background: #fff3cd;
        border: 1px solid #ffeaa7;
        border-radius: 5px;
        padding: 10px;
        margin: 10px 0;
      }
      
      code {
        background: #f1f3f4;
        padding: 2px 6px;
        border-radius: 3px;
        font-family: 'Consolas', monospace;
      }
    </style>
  </head>

  <body>
    <!-- START Part 1 -->
<div class="container">
  <div class="author-info">
    <h1>World Models: Dreamer V3, IRIS, and Beyond</h1>
    <h2 style="color:white;border:none;margin:10px 0;font-size:1.3em;">
      Deep Model-Based Reinforcement Learning Explained in Depth
    </h2>
    <p>
      <strong>Ayeen Poostforoushan</strong>
      (<a href="mailto:ayeen.pf@gmail.com" style="color:#f1c40f;">ayeen.pf@gmail.com</a>)<br>
      <strong>Kooshan Fattah</strong>
      (<a href="mailto:amirfattah5@gmail.com" style="color:#f1c40f;">amirfattah5@gmail.com</a>)<br>
      Sharif University of Technology
    </p>
  </div>

  <iframe src="images/poster.pdf" width="100%" height="600px" style="border:none;"></iframe>

  <h2>Why the World-Model Idea Matters</h2>
  <p>
    Reinforcement learning (RL) gives us a mathematical language for sequential decision-making:
    an agent interacts with an environment, receives observations and rewards,
    and gradually improves its policy to maximize long-term return.
    Yet the remarkable progress of model-free methods such as DQN, PPO, SAC, and Rainbow
    hides a sobering fact: they are <strong>extraordinarily data-hungry</strong>.
    The original Atari DQN agent, for example, required roughly fifty million frames—
    the equivalent of more than two hundred hours of gameplay—to approach human skill.
    Robots trained with PPO may spend months of real time before mastering a basic locomotion skill.
  </p>
  <p>
    Humans and animals do not learn this way.
    We mentally simulate: a child learning to stack blocks imagines what will happen if a tower leans too far,
    a driver predicts how other cars will respond to a lane change,
    and even simple creatures such as rats exhibit <em>vicarious trial and error</em>
    when deciding which tunnel to explore.
    We are constantly building and querying an <strong>internal model of the world</strong>.
    This simple observation is the seed of the world-model revolution in RL.
  </p>
  <p>
    The central hypothesis is straightforward yet profound:
    if an agent can learn an accurate probabilistic model of its environment
    \[p(s_{t+1}\mid s_t,a_t),\qquad p(o_t\mid s_t),\qquad p(r_t\mid s_t)\]
    and perform planning or policy optimization inside that learned model,
    it can achieve the same or better behavior with a tiny fraction of real-world interaction.
    Every advance described below—from Sutton’s Dyna-Q to Dreamer V3 and IRIS—builds on this core idea.
  </p>

  <div class="section-divider"></div>

  <h2>Dyna-Q: the Conceptual Starting Point</h2>
  <p>
    Long before deep networks or GPUs, <strong>Richard Sutton</strong>
    proposed a simple but powerful scheme called <strong>Dyna-Q</strong> (1990).
    Its setting was the classical Markov decision process
    \((S,A,T,R,\gamma)\):
    states \(S\), actions \(A\), transition probabilities \(T\),
    reward function \(R\), and discount factor \(\gamma\).
    The goal was still to learn an optimal action-value function
    \[
      Q^\*(s,a) = \max_\pi
      \mathbb{E}\!\left[\sum_{t=0}^\infty \gamma^t r_t
      \;\middle|\; s_0=s,\;a_0=a,\;\pi\right].
    \]
  </p>
  <p>
    Dyna-Q’s insight was to <em>learn a simple environment model on the fly</em>
    and to use it for extra practice between real interactions.
    Each time the agent experienced a transition \((s,a,r,s')\),
    it updated two things:
    the Q-table entry \(Q(s,a)\) and a one-step model
    \(M(s,a)\mapsto(s',r)\).
    Between environment steps it repeatedly
    <q>dreamed</q> by sampling state–action pairs from its experience
    and applying Q-learning updates using the model’s predictions.
    In pseudo-code:
  </p>
  <div class="algorithm-box">
    <div class="algorithm-title">Dyna-Q Algorithm</div>
    Initialize Q-table and model M<br>
    For each real interaction:  
    &nbsp;&nbsp;• Update Q with Q-learning  
    &nbsp;&nbsp;• Update model M  
    &nbsp;&nbsp;• For n planning steps: sample (s,a), query M, update Q again
  </div>
  <p>
    This was a revelation: <strong>planning and learning could be unified</strong>.
    A small number of real experiences could be amplified into a much larger
    synthetic data set simply by replaying imagined transitions.
  </p>

  <h3>Power and Limits</h3>
  <p>
    Dyna-Q was an elegant demonstration of imagination in reinforcement learning,
    but it lived in a world of lookup tables and discrete states.
    Its model \(M\) was nothing more than a dictionary mapping state–action pairs to outcomes.
    As soon as environments became continuous, high-dimensional, or partially observable,
    this representation collapsed.
    The core idea survived, however, and would reappear decades later
    dressed in deep networks and variational inference.
  </p>

  <div class="key-insight">
    <strong>Key conceptual inheritance:</strong>
    The modern Dreamer and IRIS agents can be viewed
    as very large, differentiable, neural versions of Dyna-Q’s tiny model,
    learning in latent spaces instead of tables
    and generating billions of imagined transitions per day on modern hardware.
  </div>
<!-- END Part 1 -->
<!-- START Part 2 -->
  <div class="section-divider"></div>

  <h2>The 2018 Breakthrough: Ha &amp; Schmidhuber’s World Models</h2>
  <p>
    Nearly three decades after Dyna-Q, the idea of a learned internal simulator
    was reborn in a dramatically richer context.
    In <strong>World Models</strong> (2018),
    David Ha and Jürgen Schmidhuber showed how deep generative modeling could
    turn raw video-game pixels into a predictive latent space where an agent
    can <q>dream</q> entire trajectories.
  </p>
  <p>
    The architecture separated perception, dynamics, and decision making:
  </p>
  <ul>
    <li>
      A <strong>Variational Autoencoder (VAE)</strong>
      encodes each high-dimensional frame \(x_t\) into a small latent vector \(z_t\),
      trained with the evidence lower bound
      \[
        \mathcal{L}_{VAE}
        = \mathbb{E}_{q_\phi(z\mid x)}[\log p_\theta(x\mid z)]
          - \beta \, KL[q_\phi(z\mid x) \| p(z)].
      \]
      The prior \(p(z)=\mathcal{N}(0,I)\) and the KL penalty
      encourage the latent space to be smooth and well structured.
    </li>
    <li>
      A <strong>Mixture Density RNN (MDN-RNN)</strong>
      predicts a distribution over the next latent state,
      \[
        p(z_{t+1}\mid z_t,a_t)
          = \sum_k \pi_k \,\mathcal{N}\big(\mu_k,\sigma_k^2\big),
      \]
      where the LSTM’s hidden state captures temporal context.
      By mixing Gaussians, it can model the multimodal uncertainty of video-game futures.
    </li>
    <li>
      A simple linear <strong>controller</strong>
      maps \([z_t,h_t]\) to an action
      \[
        a_t = W_c [z_t,h_t] + b_c.
      \]
      Instead of gradients, the controller’s parameters are evolved with
      Covariance Matrix Adaptation Evolution Strategy (CMA-ES),
      since backpropagating through a stochastic world model is awkward.
    </li>
  </ul>
  <p>
    These three components were trained in separate phases:
    first the VAE on collected random rollouts,
    then the MDN-RNN on latent sequences,
    and finally the controller entirely inside the learned world.
    The result was striking: an agent could reach
    competitive performance in CarRacing and
    VizDoom tasks without ever seeing new real frames during policy optimization.
  </p>

  <h3>Conceptual Lessons</h3>
  <p>
    Ha and Schmidhuber proved that
    <strong>accurate latent dynamics are enough for control</strong>.
    But they also exposed the price of modularity:
    since the three modules were trained separately and the policy never saw raw pixels,
    the system could not fine-tune representations end-to-end,
    and evolution strategies remained sample-inefficient.
    These shortcomings motivated the next leap forward.
  </p>

  <div class="section-divider"></div>

  <h2>The Dreamer Family: Differentiable Imagination</h2>
  <p>
    <strong>Dreamer</strong> (2019–2020) by Danijar Hafner and collaborators
    answered the call for end-to-end differentiable world models.
    Instead of discrete training phases,
    Dreamer jointly trains a compact latent model and an actor-critic policy
    by performing policy optimization inside the model itself.
    The core is the <strong>Recurrent State-Space Model (RSSM)</strong>.
  </p>
  <p>
    The RSSM introduces a deterministic hidden state \(h_t\)
    alongside a stochastic latent state \(z_t\):
    \[
      h_t = f_\theta(h_{t-1},z_{t-1},a_{t-1}),\qquad
      z_t \sim q_\phi(z_t \mid h_t,x_t).
    \]
    The decoder reconstructs observations and predicts rewards,
    and a KL-regularized objective
    \[
      \mathcal{L}_{model} =
      \mathbb{E}\!\left[
        \log p_\theta(x_t\mid h_t,z_t)
        - KL\big[q_\phi(z_t\mid h_t,x_t)\|p_\theta(z_t\mid h_t)\big]
      \right]
    \]
    ensures that the learned latent dynamics match real environment trajectories.
  </p>
  <p>
    Crucially, Dreamer performs <strong>imagined rollouts</strong>
    in latent space:
    starting from a real latent pair \((h,z)\),
    it samples actions from the current policy,
    predicts the next deterministic and stochastic states,
    and obtains imagined rewards and values
    entirely without new environment frames.
    Because the latent transitions are differentiable,
    gradients flow through these imagined trajectories,
    allowing the actor and critic to be updated with standard policy-gradient methods.
  </p>
  <div class="algorithm-box">
    <div class="algorithm-title">Dreamer Training Loop</div>
    1. Collect real episodes with the current policy.<br>
    2. Update the world model on the replay buffer.<br>
    3. Starting from latent states, imagine rollouts of horizon \(H\).<br>
    4. Optimize actor and critic on these imagined trajectories.<br>
    5. Repeat imagination and policy updates many times per real step.
  </div>
  <p>
    The effect is dramatic: every real step seeds many cheap imagined steps,
    driving up sample efficiency without sacrificing gradient-based learning.
  </p>

  <h3>From Dreamer to Dreamer V2</h3>
  <p>
    Dreamer V2 refined the latent representation by replacing Gaussian
    latents with <strong>categorical discrete variables</strong>.
    Using a straight-through Gumbel-Softmax estimator,
    it preserved differentiability while avoiding the <em>posterior collapse</em>
    sometimes seen with continuous VAEs.
    Discrete codes proved more stable and expressive,
    letting each category specialize in different aspects of the environment.
  </p>

  <div class="section-divider"></div>

  <h2>Dreamer V3: Robust, Domain-General World Models</h2>
  <p>
    The third generation, <strong>Dreamer V3</strong> (2023),
    brought the approach to full maturity,
    achieving human-level or better scores on 55 Atari games
    and excelling on difficult continuous-control tasks such as
    quadruped locomotion and pixel-based manipulation.
    It introduces several key innovations that stabilize training
    and extend performance across diverse reward scales and domains.
  </p>
  <ul>
    <li>
      <strong>Symlog predictions</strong>
      transform values and rewards with
      \(\operatorname{symlog}(x)=\operatorname{sign}(x)\ln(|x|+1)\),
      linear for small \(|x|\) and logarithmic for large,
      taming gradient explosions from rare but high-magnitude rewards.
    </li>
    <li>
      <strong>Free-bits regularization</strong>
      ensures every latent dimension carries at least a minimum amount of information
      by modifying the KL loss to
      \[KL_{\text{free}}[q\|p] = \max(\beta \cdot KL[q\|p], \text{free bits}).\]
      This guards against degenerate latents and posterior collapse.
    </li>
    <li>
      A <strong>robust replay buffer</strong>
      carefully balances on-policy freshness with off-policy diversity,
      allowing the model to learn long-horizon dependencies without catastrophic forgetting.
    </li>
  </ul>
  <p>
    The result is a system that can seamlessly handle
    everything from proprioceptive continuous control to rich visual Atari domains,
    setting new state-of-the-art results while remaining computationally practical.
  </p>

  <div class="key-insight">
    <strong>Takeaway:</strong>
    With Dreamer V3, the vision of Dyna-Q is fully realized:
    an agent can gather modest real experience, learn a compact predictive model,
    and improve its policy almost entirely through internally generated trajectories,
    with gradient signals propagating through every component.
  </div>
<!-- END Part 2 -->
<!-- START Part 3 -->
  <div class="section-divider"></div>

  <h2>IRIS: Transformers as World Models</h2>
  <p>
    The most recent leap in this lineage is <strong>IRIS</strong> (2023–2025),
    which replaces recurrent dynamics with a <strong>transformer world model</strong>.
    Transformers have revolutionized natural language processing and computer vision
    by capturing long-range dependencies through self-attention.
    IRIS shows they are equally powerful for modelling environment dynamics.
  </p>
  <p>
    The first stage of IRIS is a <strong>discrete autoencoder</strong>
    (akin to a VQ-VAE), which converts every input image into a grid of tokens:
    \[
      E_\phi : \mathbb{R}^{H\times W\times C}\rightarrow \{1,\dots,K\}^{h\times w}.
    \]
    Each token is an index into a learned codebook
    \(\mathbf{e}\in\mathbb{R}^{K\times d}\).
    This tokenization is crucial: instead of modeling pixels directly,
    IRIS predicts the evolution of a discrete sequence of symbols.
  </p>
  <p>
    The transformer then treats the evolving trajectory
    \((s_0,a_0,r_0,s_1,\dots)\)
    as a single long sequence of tokens and predicts its continuation:
    \[
      p(\mathbf{z}_{t+1}\mid \mathbf{z}_{\le t})
      = \text{Transformer}(\mathbf{z}_{\le t}).
    \]
    Because self-attention sees every past token at once,
    it can model long-horizon dependencies that RNNs compress into a single vector.
  </p>
  <p>
    IRIS generates imagined rollouts by autoregressively sampling future tokens,
    alternating between action, reward, and next-state symbols:
  </p>
  <div class="algorithm-box">
    <div class="algorithm-title">IRIS Imagination Rollout</div>
    1. Start with current state tokens \(\mathbf{z}_t\).<br>
    2. Append action token \(a_t\) sampled from the policy.<br>
    3. Predict reward and next state tokens using the transformer.<br>
    4. Feed them back to produce \(\mathbf{z}_{t+1}\), and repeat for horizon \(H\).
  </div>
  <p>
    Because both the policy \(\pi_\theta(a|\mathbf{s})\) and value function
    \(V_\psi(\mathbf{s})\) operate directly on the same discrete token space,
    the entire system remains end-to-end differentiable and easy to scale.
    Increasing the transformer’s width, depth, or context length
    translates directly into better predictive and control performance,
    just as in large language models.
  </p>

  <h3>Advantages Over RNN-Based World Models</h3>
  <ul>
    <li>
      <strong>Long-range planning:</strong>
      Attention enables explicit connections across hundreds of steps,
      supporting multi-minute strategies that would strain an RNN’s memory.
    </li>
    <li>
      <strong>Scalability and transfer:</strong>
      The same architecture can leverage decades of research on transformer training,
      distributed optimization, and model scaling.
    </li>
    <li>
      <strong>Sample efficiency:</strong>
      Benchmarks such as Atari-100k report human-normalized scores above 2.0,
      exceeding Dreamer V3 while using fewer environment interactions.
    </li>
    <li>
      <strong>Modality agnosticism:</strong>
      Any sensor—images, audio, proprioception—can be tokenized and added to the same sequence.
    </li>
  </ul>
  <p>
    In short, IRIS demonstrates that the transformer revolution extends far beyond language and vision,
    making it a natural backbone for world models in robotics, games, and potentially real-world agents.
  </p>

  <div class="section-divider"></div>

  <h2>Architectural Evolution and Deep Comparison</h2>
  <p>
    The progression from Dyna-Q through Dreamer to IRIS
    is not merely chronological but conceptual.
    Each generation attacks a fundamental bottleneck in sample-efficient decision making.
  </p>
  <table>
    <tr>
      <th>Generation</th>
      <th>Representation</th>
      <th>Temporal Model</th>
      <th>Policy Learning</th>
      <th>Signature Innovation</th>
    </tr>
    <tr>
      <td>Dyna-Q (1990)</td>
      <td>Tabular states</td>
      <td>Lookup table</td>
      <td>Q-learning + planning</td>
      <td>Unifying model learning and planning</td>
    </tr>
    <tr>
      <td>World Models (2018)</td>
      <td>Continuous VAE latent</td>
      <td>MDN-RNN</td>
      <td>Evolution Strategies</td>
      <td>Latent imagination from pixels</td>
    </tr>
    <tr>
      <td>Dreamer V1/V2</td>
      <td>Gaussian / categorical latents</td>
      <td>RSSM (recurrent state-space)</td>
      <td>Actor-Critic with imagined rollouts</td>
      <td>End-to-end differentiable world models</td>
    </tr>
    <tr>
      <td>Dreamer V3</td>
      <td>Categorical + symlog scaling</td>
      <td>RSSM with free-bits regularization</td>
      <td>Robust actor-critic</td>
      <td>Stability and domain generalization</td>
    </tr>
    <tr>
      <td>IRIS</td>
      <td>Discrete tokens</td>
      <td>Transformer self-attention</td>
      <td>Transformer-based actor-critic</td>
      <td>Long-horizon, scalable imagination</td>
    </tr>
  </table>
  <p>
    The lesson is clear:
    richer representations (from tabular to discrete tokens),
    more expressive temporal models (from lookup tables to transformers),
    and tighter policy–model integration
    have yielded exponential gains in sample efficiency and capability.
  </p>

  <div class="section-divider"></div>

  <h2>Beyond Benchmarks: Challenges and Open Problems</h2>
  <p>
    Despite spectacular benchmark results,
    world models face unsolved challenges that keep them at the research frontier.
  </p>
  <h3>Model Bias and Distribution Shift</h3>
  <p>
    A learned world model is never perfect.
    Small systematic errors can accumulate during long imagined rollouts,
    leading to <em>model bias</em>—policies that exploit inaccuracies rather than true dynamics.
    Distribution shift exacerbates this:
    as the policy improves, it visits novel states not well-covered in the replay buffer,
    where the model may be unreliable.
    Techniques such as conservative value estimation, adaptive horizon selection,
    or uncertainty-aware planning remain active research areas.
  </p>
  <h3>Exploration and Representation Learning</h3>
  <p>
    Learning a rich latent space is not automatic.
    Sparse-reward tasks still pose difficulties because the model
    may fail to represent rare but crucial states.
    Intrinsic motivation, curiosity-driven objectives, and contrastive self-supervision
    are promising but not yet fully solved at scale.
  </p>
  <h3>Computation and Energy</h3>
  <p>
    While world models reduce <em>environment</em> samples,
    they often increase <em>compute</em> due to large neural networks
    and heavy imagination workloads.
    Training a high-capacity Dreamer or IRIS model can require dozens of GPUs or TPUs.
    Finding architectures that are both sample- and compute-efficient is essential
    for deployment on edge robots and embedded systems.
  </p>

  <div class="section-divider"></div>

  <h2>Future Directions</h2>
  <p>
    The poster rightly emphasizes that today’s achievements
    are just the beginning.
    Several exciting research frontiers are already visible:
  </p>
  <ul>
    <li>
      <strong>Multimodal world models:</strong>
      Future agents must integrate vision, audition, proprioception, and natural language.
      Token-based transformers like IRIS are naturally suited to fuse such diverse streams.
    </li>
    <li>
      <strong>Hierarchical planning:</strong>
      Combining world models with goal-setting or option frameworks
      could enable reasoning over days or weeks of simulated time.
      A two-level policy
      \[
        g_t\sim\pi_{\text{high}}(g|s_t),\qquad
        a_t\sim\pi_{\text{low}}(a|s_t,g_t)
      \]
      is one promising path.
    </li>
    <li>
      <strong>Meta-learning and few-shot adaptation:</strong>
      Agents should rapidly adapt their internal model to new environments
      without long retraining.
      In-context adaptation and model-agnostic meta-learning are key techniques.
    </li>
    <li>
      <strong>Theoretical foundations:</strong>
      We still lack sharp guarantees on how model accuracy translates
      into policy performance and on the sample complexity of imagined updates.
    </li>
  </ul>
  <p>
    As these directions mature, world models may become the
    standard backbone for truly general-purpose intelligent agents.
  </p>

  <div class="section-divider"></div>

  <h2>Concluding Perspective</h2>
  <p>
    From the tabular updates of Dyna-Q to the transformer imagination of IRIS,
    world models chart a thirty-five-year arc of deepening insight.
    Each generation kept the spirit of Dyna-Q—learning an internal simulator
    and improving policy by planning inside it—while steadily
    increasing representational richness, training stability, and domain coverage.
  </p>
  <p>
    Today’s best agents learn a compact latent universe,
    roll out thousands of possible futures per second,
    and use those dreams to act wisely in the real world.
    The result is a dramatic improvement in sample efficiency,
    a unification of planning and learning,
    and a glimpse of general intelligence.
    As compute and algorithms continue to improve,
    the boundary between imagined and real experience will blur even further.
  </p>

  <h2>References</h2>
  <p>
    Sutton, R. (1990). <em>Dyna: An Integrated Architecture for Learning and Planning</em>.<br>
    Ha, D. &amp; Schmidhuber, J. (2018). <em>World Models</em>.<br>
    Hafner, D. et al. (2020). <em>Dream to Control</em>.<br>
    Hafner, D. et al. (2023). <em>Mastering Diverse Domains through World Models</em>.<br>
    Micheli, V. et al. (2023). <em>Transformers are Sample-Efficient World Models</em>.
  </p>
</div>
<!-- END Part 3 -->

  </body>
</html>